{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tscv_sliding\n",
    "from window_generator import WindowGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fredapi import Fred\n",
    "fred_api_key = \"29b81578246f3b1d8661dfdb956124ba\"\n",
    "fred = Fred(api_key=fred_api_key)\n",
    "start_date = \"2003-01-02\" \n",
    "end_date = \"2022-12-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vix = fred.get_series(\"VIXCLS\", observation_start=start_date, observation_end=end_date).to_frame()\n",
    "vix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vix.rename(columns={0:\"VIX\"}, inplace=True)\n",
    "vix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vix.fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(vix)\n",
    "n_train = int(n*0.7)\n",
    "n_val = int(n*0.9)\n",
    "train_df = vix[0:n_train]\n",
    "val_df = vix[n_train:n_val]\n",
    "test_df = vix[n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "cols = train_df.columns\n",
    "train_indices = train_df.index\n",
    "val_indices = val_df.index\n",
    "test_indices = test_df.index\n",
    "\n",
    "scale = MinMaxScaler()\n",
    "train_df = scale.fit_transform(train_df)\n",
    "val_df = scale.transform(val_df)\n",
    "test_df = scale.transform(test_df)\n",
    "\n",
    "train_df = pd.DataFrame(train_df, columns=cols, index=train_indices)\n",
    "val_df = pd.DataFrame(val_df, columns=cols, index=val_indices)\n",
    "test_df = pd.DataFrame(test_df, columns=cols, index=test_indices)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "w = WindowGenerator(input_width=input_width, \n",
    "                    label_width=1,\n",
    "                    shift=1,\n",
    "                    train_df=train_df,\n",
    "                    val_df=val_df,\n",
    "                    test_df=test_df,\n",
    "                    n_splits=5,\n",
    "                    train_splits=3,\n",
    "                    test_splits=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlike(y_true, y_pred):\n",
    "  return tf.math.log(y_pred) + (y_true / y_pred)\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, return_sequences=True, input_shape=(input_width, 1))) #(timesteps, features)\n",
    "    model.add(LSTM(96, return_sequences=True)) #96 #192 #160\n",
    "    #model.add(LSTM(256, return_sequences=True)) # #256 #(192)\n",
    "    model.add(LSTM(64)) #64 \n",
    "    model.add(Dropout(0.2)) #0.2 #0.1 #0.2\n",
    "    model.add(Dense(1, activation=\"relu\"))\n",
    "    \n",
    "    learning_rate = 0.001 #0.001 #0.0001 #0.001\n",
    "    metrics = [\"mse\", \"mae\", \"mape\", tf.keras.metrics.RootMeanSquaredError(), qlike]\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=learning_rate), metrics = metrics)\n",
    "    return model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 50\n",
    "model = build_model()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_mse', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(w.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=w.val,\n",
    "                      callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_width = 200\n",
    "shift = ar_width - input_width\n",
    "\n",
    "test = np.concatenate([x for x, y in w.test], axis=0)\n",
    "predictions = model.predict(test)\n",
    "y = np.concatenate([y for x, y in w.test], axis=0).reshape(-1, 1)\n",
    "\n",
    "predicted = scale.inverse_transform(predictions).flatten()[shift:]\n",
    "actual = scale.inverse_transform(y).flatten()[shift:]\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "timesteps = np.array([i for  i in range(len(predicted))])\n",
    "n = 322\n",
    "sns.lineplot(ax = ax, x = timesteps[0:n], y = predicted[0:n], label = 'LSTM')\n",
    "sns.lineplot(ax = ax, x = timesteps[0:n], y = actual[0:n], label = 'VIX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse =  mean_squared_error(actual, predicted)\n",
    "mae =  mean_absolute_error(actual, predicted)\n",
    "\n",
    "print(\"MSE: \", mse)\n",
    "print(\"MAE: \", mae)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Econometric benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width_ar = ar_width\n",
    "\n",
    "ar_w = WindowGenerator(input_width=input_width_ar, \n",
    "                    label_width=1,\n",
    "                    shift=1,\n",
    "                    train_df=train_df,\n",
    "                    val_df=val_df,\n",
    "                    test_df=test_df,\n",
    "                    n_splits=3,\n",
    "                    train_splits=3,\n",
    "                    test_splits=1,\n",
    "                    scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in ar_w.test], axis=0)\n",
    "x_test = np.concatenate([x for x, y in ar_w.test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "predictions = []\n",
    "resid = []\n",
    "\n",
    "for x, y in zip(x_test, y_test):\n",
    "    arima_model = AutoReg(endog=x, lags=3).fit()\n",
    "    day_ahead = x_test.shape[1] + 1\n",
    "    predictions.append(arima_model.predict(day_ahead, day_ahead))\n",
    "    resid.append(y[0] - arima_model.predict(day_ahead, day_ahead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = np.arange(1, len(predictions) + 1)\n",
    "\n",
    "y = scale.inverse_transform(y_test.reshape((len(predictions), -1)))\n",
    "predictions = scale.inverse_transform(np.array(predictions))\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "dates = test_indices[input_width:]\n",
    "print(dates.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (25,10))\n",
    "\n",
    "n = 322 # How many days to see in plot\n",
    "\n",
    "plt.plot(dates[:n], predictions[:n], label = \"AR(3)\")\n",
    "plt.plot(dates[:n], y[:n], label = \"VIX\")\n",
    "plt.title(\"Out of sample forecasts: one day ahead\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y, predictions)\n",
    "mae = mean_absolute_error(y, predictions)\n",
    "\n",
    "print(\"MSE: \", mse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "negloglik = lambda y, rv_y: -rv_y.log_prob(y)\n",
    "\n",
    "n_samples = np.concatenate([x for x, y in w.train], axis=0).shape[0]\n",
    "\n",
    "# Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.\n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "  n = kernel_size + bias_size\n",
    "  c = np.log(np.expm1(1.))\n",
    "  return tf.keras.Sequential([\n",
    "      tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "          tfd.Normal(loc=t[..., :n],\n",
    "                     scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n",
    "          reinterpreted_batch_ndims=1)),\n",
    "  ])\n",
    "\n",
    "# Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\n",
    "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "  n = kernel_size + bias_size\n",
    "  return tf.keras.Sequential([\n",
    "      tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "          tfd.Normal(loc=t, scale=1),\n",
    "          reinterpreted_batch_ndims=1)),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bnn():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, return_sequences=True, input_shape=(input_width, 1))) #(timesteps, features)\n",
    "    model.add(LSTM(96, return_sequences=True)) #96 #192 #160\n",
    "    #model.add(LSTM(256, return_sequences=True)) # #256 #(192)\n",
    "    model.add(LSTM(64)) #64 \n",
    "    model.add(Dropout(0.2)) #0.2 #0.1 #0.2\n",
    "    model.add(tfp.layers.DenseVariational(1, posterior_mean_field, prior_trainable, kl_weight=1/n_samples))\n",
    "    #model.add(Dense(1, activation=\"relu\"))\n",
    "    \n",
    "    learning_rate = 0.001 #0.001 #0.0001 #0.001\n",
    "    metrics = [\"mse\", \"mae\", \"mape\", tf.keras.metrics.RootMeanSquaredError(), qlike]\n",
    "    model.compile(loss=negloglik, optimizer=Adam(learning_rate=learning_rate), metrics = metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = build_bnn()\n",
    "bnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 300\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_mse', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = bnn.fit(w.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=w.val)\n",
    "                      #callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_x = np.concatenate([x for x, y in w.test], axis = 0)\n",
    "b_y = np.concatenate([y for x, y in w.test], axis = 0)\n",
    "\n",
    "ensemble = []\n",
    "\n",
    "for i in range(100):\n",
    "    ensemble.append(bnn(b_x))\n",
    "\n",
    "ensemble = scale.inverse_transform(np.array(ensemble).reshape((100, -1)))\n",
    "\n",
    "mean = np.mean(ensemble, axis = 0)\n",
    "std = np.std(ensemble, axis = 0)\n",
    "\n",
    "b_y = scale.inverse_transform(b_y.reshape((len(mean), -1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(b_y, mean)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = vix[n_val:]\n",
    "\n",
    "mean = mean.flatten()\n",
    "b_y = b_y.flatten()\n",
    "\n",
    "\n",
    "preds_interval = pd.DataFrame(ensemble.T, index = test_indices[30:])\n",
    "preds_interval['Date'] = test_indices[30:]\n",
    "preds_interval = preds_interval.melt(id_vars = 'Date', var_name = 'Labels', value_name = 'Vals')\n",
    "\n",
    "timesteps = np.array([i for  i in range(len(mean))])\n",
    "\n",
    "actual_interval = pd.DataFrame([test_indices[30:], b_y]).T\n",
    "actual_interval.columns = [\"Date\", \"Vals\"]\n",
    "\n",
    "\n",
    "n = 300\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.lineplot(ax = ax, data = preds_interval, x = 'Date', y = 'Vals', color = \"r\", label=\"BNN\", errorbar=(\"sd\"))\n",
    "sns.lineplot(ax = ax, data = test_df)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = np.abs(b_y - mean)\n",
    "\n",
    "sns.regplot(x = resid, y = std)\n",
    "plt.xlabel(\"Prediction error\")\n",
    "plt.ylabel(\"Prediction standard deviation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorrun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
